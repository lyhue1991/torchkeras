{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ—¥å¿—åˆ·å±ä½¿æˆ‘ç—›è‹¦ï¼Œæˆ‘å¼€å‘äº†VLogğŸ˜‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®­ç»ƒæ—¥å¿—åˆ·å±ä½¿æˆ‘ç—›è‹¦ï¼Œæˆ‘å¼€å‘äº†VLogï¼Œå¯ä»¥åœ¨ä»»æ„è®­ç»ƒä»£ç ä¸­è½»æ¾ä½¿ç”¨~\n",
    "\n",
    "ä¾‹å¦‚ï¼Œé€šè¿‡å›è°ƒåµŒå…¥åˆ°lightgbm/catboost/transformers/ultralyticsï¼Œä¹ƒè‡³kerasåº“çš„è®­ç»ƒä»£ç æµç¨‹ä¸­~\n",
    "\n",
    "\n",
    "**before**:\n",
    "\n",
    "![](./data/before.png)\n",
    "\n",
    "\n",
    "\n",
    "**after**ï¼š\n",
    "\n",
    "![](./data/torchkeras_plot.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ä¸ºä»€ä¹ˆä¸ç”¨tensorboardæˆ–è€…wandb?**\n",
    "\n",
    "tensorboardéœ€è¦å¼€ç«¯å£æƒé™ï¼ŒæœåŠ¡å™¨å¼€å‘ç¯å¢ƒæœ‰æ—¶å€™æ²¡æœ‰ç«¯å£æƒé™~\n",
    "\n",
    "wandbéœ€è¦è”ç½‘ï¼Œæœ‰æ—¶å€™ç½‘é€Ÿå¾ˆå·®æˆ–è€…æ²¡æœ‰ç½‘ï¼Œå½±å“ä½“éªŒ~\n",
    "\n",
    "ç»¼åˆå¯¹æ¯”è€ƒè™‘å¦‚ä¸‹è¡¨\n",
    "\n",
    "|æ—¥å¿—æ–¹æ¡ˆ    | å­¦ä¹ æˆæœ¬|  æŒ‡æ ‡ç›´è§‚æ€§ |æ˜¯å¦éœ€è¦ç«¯å£æƒé™|æ˜¯å¦éœ€è¦è”ç½‘ |æ¨èæ˜Ÿçº§|\n",
    "|:---------|:------:|:-------------:|:---------:|:---------:|:-----|\n",
    "|print      | æ— éœ€å­¦ä¹  |ä¸ç›´è§‚,æ˜“åˆ·å±|ä¸éœ€è¦    |ä¸éœ€è¦     |â­ï¸|\n",
    "|tensorboard| è¾ƒéš¾å­¦ä¹  |æ¯”è¾ƒç›´è§‚,è·¨é¡µé¢      |éœ€è¦       |ä¸éœ€è¦     |â­ï¸â­ï¸|\n",
    "|wandb      | è¾ƒå¥½å­¦ä¹  |æ¯”è¾ƒç›´è§‚,è·¨é¡µé¢      |ä¸éœ€è¦     |éœ€è¦       |â­ï¸â­ï¸â­ï¸â­ï¸|\n",
    "|VLogğŸ˜‹     | ææ˜“å­¦ä¹  |éå¸¸ç›´è§‚,åŒé¡µé¢      |ä¸éœ€è¦     |ä¸éœ€è¦      |â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U lightgbm \n",
    "#!pip install ultralytics\n",
    "#!pip install git+https://github.com/lyhue1991/torchkeras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ï¼ŒVLogåŸºæœ¬åŸç†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VLogç±»ä¸»è¦æœ‰ä»¥ä¸‹5ä¸ªæ–¹æ³•ã€‚\n",
    "\n",
    "```python\n",
    "\n",
    "from torchkeras import VLog\n",
    "\n",
    "#1, åˆå§‹åŒ–æ–¹æ³•\n",
    "vlog = VLog(epochs=20, monitor_metric='val_loss', monitor_mode='min') \n",
    "\n",
    "#2, æ˜¾ç¤ºå¼€å§‹ç©ºå›¾è¡¨\n",
    "vlog.log_start()\n",
    "\n",
    "#3, æ›´æ–°stepçº§åˆ«æ—¥å¿—\n",
    "vlog.log_step({'train_loss':0.003,'val_loss':0.002}) \n",
    "\n",
    "#4, æ›´æ–°epochçº§åˆ«æ—¥å¿—\n",
    "vlog.log_epoch({'train_acc':0.9,'val_acc':0.87,'train_loss':0.002,'val_loss':0.03})\n",
    "\n",
    "#5, è¾“å‡ºæœ€ç»ˆç¨³å®šçŠ¶æ€å›¾è¡¨\n",
    "vlog.log_end()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math,random\n",
    "from torchkeras import VLog\n",
    "\n",
    "epochs = 10\n",
    "batchs = 30\n",
    "\n",
    "#0, æŒ‡å®šç›‘æ§åŒ—ææ˜ŸæŒ‡æ ‡ï¼Œä»¥åŠæŒ‡æ ‡ä¼˜åŒ–æ–¹å‘\n",
    "vlog = VLog(epochs, monitor_metric='val_loss', monitor_mode='min') \n",
    "\n",
    "#1, log_start åˆå§‹åŒ–åŠ¨æ€å›¾è¡¨\n",
    "vlog.log_start() \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #train\n",
    "    for step in range(batchs):\n",
    "        \n",
    "        #2, log_step æ›´æ–°stepçº§åˆ«æ—¥å¿—ä¿¡æ¯ï¼Œæ‰“æ—¥å¿—ï¼Œå¹¶ç”¨å°è¿›åº¦æ¡æ˜¾ç¤ºè¿›åº¦\n",
    "        vlog.log_step({'train_loss':100-2.5*epoch+math.sin(2*step/batchs)}) \n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    #eval    \n",
    "    for step in range(20):\n",
    "        \n",
    "        #3, log_step æ›´æ–°stepçº§åˆ«æ—¥å¿—ä¿¡æ¯ï¼ŒæŒ‡å®štraining=Falseè¯´æ˜åœ¨éªŒè¯æ¨¡å¼ï¼Œåªæ‰“æ—¥å¿—ä¸æ›´æ–°å°è¿›åº¦æ¡\n",
    "        vlog.log_step({'val_loss':100-2*epoch+math.sin(2*step/batchs)},training=False)\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "    #4, log_epoch æ›´æ–°epochçº§åˆ«æ—¥å¿—ä¿¡æ¯ï¼Œæ¯ä¸ªepochåˆ·æ–°ä¸€æ¬¡åŠ¨æ€å›¾è¡¨å’Œå¤§è¿›åº¦æ¡è¿›åº¦\n",
    "    vlog.log_epoch({'val_loss':100 - 2*epoch+2*random.random()-1,\n",
    "                    'train_loss':100-2.5*epoch+2*random.random()-1})  \n",
    "\n",
    "# 5, log_end è°ƒæ•´åæ ‡è½´èŒƒå›´ï¼Œè¾“å‡ºæœ€ç»ˆæŒ‡æ ‡å¯è§†åŒ–å›¾è¡¨\n",
    "vlog.log_end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒï¼Œåœ¨LightGBMä¸­ä½¿ç”¨VLog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®¾è®¡ä¸€ä¸ªç®€å•çš„å›è°ƒï¼Œå°±å¯ä»¥æå®š~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import VLog\n",
    "class VLogCallback:\n",
    "    def __init__(self, num_boost_round, \n",
    "                 monitor_metric='val_loss',\n",
    "                 monitor_mode='min'):\n",
    "        self.order = 20\n",
    "        self.num_boost_round = num_boost_round\n",
    "        self.vlog = VLog(epochs = num_boost_round, monitor_metric = monitor_metric, \n",
    "                         monitor_mode = monitor_mode)\n",
    "\n",
    "    def __call__(self, env) -> None:\n",
    "        metrics = {}\n",
    "        for item in env.evaluation_result_list:\n",
    "            if len(item) == 4:\n",
    "                data_name, eval_name, result = item[:3]\n",
    "                metrics[data_name+'_'+eval_name] = result\n",
    "            else:\n",
    "                data_name, eval_name = item[1].split()\n",
    "                res_mean = item[2]\n",
    "                res_stdv = item[4]\n",
    "                metrics[data_name+'_'+eval_name] = res_mean\n",
    "        self.vlog.log_epoch(metrics)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def printlog(info):\n",
    "    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(\"\\n\"+\"==========\"*8 + \"%s\"%nowtime)\n",
    "    print(info+'...\\n\\n')\n",
    "\n",
    "#================================================================================\n",
    "# ä¸€ï¼Œè¯»å–æ•°æ®\n",
    "#================================================================================\n",
    "printlog(\"step1: reading data...\")\n",
    "\n",
    "# è¯»å–dftrain,dftest\n",
    "breast = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(breast.data,columns = [x.replace(' ','_') for x in breast.feature_names])\n",
    "df['label'] = breast.target\n",
    "df['mean_radius'] = df['mean_radius'].apply(lambda x:int(x))\n",
    "df['mean_texture'] = df['mean_texture'].apply(lambda x:int(x))\n",
    "dftrain,dftest = train_test_split(df)\n",
    "\n",
    "categorical_features = ['mean_radius','mean_texture']\n",
    "lgb_train = lgb.Dataset(dftrain.drop(['label'],axis = 1),label=dftrain['label'],\n",
    "                        categorical_feature = categorical_features)\n",
    "\n",
    "lgb_valid = lgb.Dataset(dftest.drop(['label'],axis = 1),label=dftest['label'],\n",
    "                        categorical_feature = categorical_features,\n",
    "                        reference=lgb_train)\n",
    "\n",
    "#================================================================================\n",
    "# äºŒï¼Œè®¾ç½®å‚æ•°\n",
    "#================================================================================\n",
    "printlog(\"step2: setting parameters...\")\n",
    "                               \n",
    "boost_round = 50                   \n",
    "early_stop_rounds = 10\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective':'binary',\n",
    "    'metric': ['auc'], #'l2'\n",
    "    'num_leaves': 15,   \n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0,\n",
    "    'early_stopping_round':5\n",
    "}\n",
    "\n",
    "#================================================================================\n",
    "# ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step3: training model...\")\n",
    "\n",
    "result = {}\n",
    "\n",
    "vlog_cb = VLogCallback(boost_round, monitor_metric = 'val_auc', monitor_mode = 'max')\n",
    "vlog_cb.vlog.log_start()\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round= boost_round,\n",
    "                valid_sets=(lgb_valid, lgb_train),\n",
    "                valid_names=('val','train'),\n",
    "                callbacks = [lgb.record_evaluation(result),\n",
    "                             vlog_cb]\n",
    "               )\n",
    "\n",
    "vlog_cb.vlog.log_end()\n",
    "\n",
    "#================================================================================\n",
    "# å››ï¼Œè¯„ä¼°æ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step4: evaluating model ...\")\n",
    "\n",
    "y_pred_train = gbm.predict(dftrain.drop('label',axis = 1), num_iteration=gbm.best_iteration)\n",
    "y_pred_test = gbm.predict(dftest.drop('label',axis = 1), num_iteration=gbm.best_iteration)\n",
    "\n",
    "print('train accuracy: {:.5} '.format(accuracy_score(dftrain['label'],y_pred_train>0.5)))\n",
    "print('valid accuracy: {:.5} \\n'.format(accuracy_score(dftest['label'],y_pred_test>0.5)))\n",
    "\n",
    "\n",
    "#================================================================================\n",
    "# äº”ï¼Œä¿å­˜æ¨¡å‹\n",
    "#================================================================================\n",
    "printlog(\"step5: saving model ...\")\n",
    "\n",
    "\n",
    "model_dir = \"gbm.model\"\n",
    "print(\"model_dir: %s\"%model_dir)\n",
    "gbm.save_model(\"gbm.model\")\n",
    "printlog(\"task end...\")\n",
    "\n",
    "###\n",
    "##\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ï¼Œ åœ¨ultralyticsä¸­ä½¿ç”¨VLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å†™ä¸ªé€‚é…çš„å›è°ƒ~\n",
    "\n",
    "ultralyticså¯ä»¥åš åˆ†ç±»ï¼Œæ£€æµ‹ï¼Œåˆ†å‰² ç­‰ç­‰ã€‚\n",
    "\n",
    "è¿™ä¸ªå›è°ƒå‡½æ•°æ˜¯é€šç”¨çš„ï¼Œæ­¤å¤„ä»¥åˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ”¹ä¸ªmonitor_metricå³å¯~\n",
    "\n",
    "\n",
    "cats_vs_dogsæ•°æ®é›†å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤:**torchkeras** è·å–~ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import VLog\n",
    "class VLogCallback:\n",
    "    def __init__(self,epochs,monitor_metric,monitor_mode):\n",
    "        self.vlog = VLog(epochs,monitor_metric,monitor_mode)\n",
    "        \n",
    "    def on_train_batch_end(self,trainer):\n",
    "        self.vlog.log_step(trainer.label_loss_items(trainer.tloss, prefix='train'))\n",
    "\n",
    "    def on_fit_epoch_end(self,trainer):\n",
    "        metrics = {k.split('/')[-1]:v for k,v in trainer.metrics.items() if 'loss' not in k}\n",
    "        self.vlog.log_epoch(metrics)\n",
    "\n",
    "    def on_train_epoch_end(self,trainer):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO \n",
    "epochs = 10\n",
    "\n",
    "vlog_cb = VLogCallback(epochs = epochs,\n",
    "                       monitor_metric='accuracy_top1',\n",
    "                       monitor_mode='max')\n",
    "callbacks = {\n",
    "    \"on_train_batch_end\": vlog_cb.on_train_batch_end,\n",
    "    \"on_fit_epoch_end\": vlog_cb.on_fit_epoch_end\n",
    "}\n",
    "\n",
    "model = YOLO(model = 'yolov8n-cls.pt')\n",
    "for event,func in callbacks.items():\n",
    "    model.add_callback(event,func)\n",
    "    \n",
    "vlog_cb.vlog.log_start()\n",
    "results = model.train(data='cats_vs_dogs', \n",
    "                      epochs=epochs, workers=4)     # train the model\n",
    "vlog_cb.vlog.log_end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ï¼Œ åœ¨transformersä¸­ä½¿ç”¨VLog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "waimaiè¯„è®ºæ•°æ®é›†å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤:**torchkeras** è·å–~ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#å›è°ƒç»™ä½ å†™å¥½äº†~\n",
    "from torchkeras.tools.transformers import VLogCallback "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch \n",
    "import datasets \n",
    "from transformers import AutoTokenizer,DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification \n",
    "from transformers import TrainingArguments,Trainer \n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "from tqdm import tqdm \n",
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "\n",
    "#ä¸€ï¼Œå‡†å¤‡æ•°æ®\n",
    "\n",
    "df = pd.read_csv(\"waimai_10k.csv\")\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "ds = ds.shuffle(42) \n",
    "ds = ds.rename_columns({\"review\":\"text\",\"label\":\"labels\"})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese') \n",
    "\n",
    "ds_encoded = ds.map(lambda example:tokenizer(example[\"text\"]),\n",
    "                    remove_columns = [\"text\"],\n",
    "                    batched=True)\n",
    "\n",
    "#train,val,test split\n",
    "ds_train_val,ds_test = ds_encoded.train_test_split(test_size=0.2).values()\n",
    "ds_train,ds_val = ds_train_val.train_test_split(test_size=0.2).values() \n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=16, collate_fn = data_collator)\n",
    "dl_val = torch.utils.data.DataLoader(ds_val, batch_size=16,  collate_fn = data_collator)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=16,  collate_fn = data_collator)\n",
    "\n",
    "for batch in dl_train:\n",
    "    break\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "\n",
    "\n",
    "\n",
    "#äºŒï¼Œå®šä¹‰æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'bert-base-chinese',num_labels=2)\n",
    "\n",
    "#ä¸‰ï¼Œè®­ç»ƒæ¨¡å‹\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    accuracy = np.sum(preds==labels)/len(labels)\n",
    "    precision = np.sum((preds==1)&(labels==1))/np.sum(preds==1)\n",
    "    recall = np.sum((preds==1)&(labels==1))/np.sum(labels==1)\n",
    "    f1  = 2*recall*precision/(recall+precision)\n",
    "    return {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,'f1':f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"bert_waimai\",\n",
    "    num_train_epochs = 3,\n",
    "    logging_steps = 20,\n",
    "    gradient_accumulation_steps = 10,\n",
    "    evaluation_strategy=\"steps\", #epoch\n",
    "    \n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    \n",
    "    report_to='none',\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=10),\n",
    "             VLogCallback()] #ç›‘æ§æŒ‡æ ‡åŒ metric_for_best_model\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = callbacks,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train() \n",
    "\n",
    "\n",
    "\n",
    "#å››ï¼Œè¯„ä¼°æ¨¡å‹\n",
    "trainer.evaluate(ds_val)\n",
    "\n",
    "\n",
    "#äº”ï¼Œä½¿ç”¨æ¨¡å‹\n",
    "from transformers import pipeline\n",
    "model.config.id2label = {0:\"å·®è¯„\",1:\"å¥½è¯„\"}\n",
    "classifier = pipeline(task=\"text-classification\",tokenizer = tokenizer,model=model.cpu())\n",
    "classifier(\"æŒºå¥½åƒçš„å“¦\")\n",
    "\n",
    "#å…­ï¼Œä¿å­˜æ¨¡å‹\n",
    "model.save_pretrained(\"waimai_10k_bert\")\n",
    "tokenizer.save_pretrained(\"waimai_10k_bert\")\n",
    "\n",
    "classifier = pipeline(\"text-classification\",model=\"waimai_10k_bert\")\n",
    "classifier([\"å‘³é“è¿˜ä¸é”™ï¼Œä¸‹æ¬¡å†æ¥\",\"æˆ‘å»ï¼Œåƒäº†æˆ‘åäº†ä¸‰å¤©\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœåœ¨torchkerasçš„ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åœ¨é¡¹ç›®ä¸­æäº¤issueã€‚\n",
    "\n",
    "å¦‚æœæƒ³è¦è·å¾—æ›´å¿«çš„åé¦ˆæˆ–è€…ä¸å…¶ä»–torchkerasç”¨æˆ·å°ä¼™ä¼´è¿›è¡Œäº¤æµï¼Œ\n",
    "\n",
    "å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
