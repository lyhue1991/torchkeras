### DQNè§£å†³å€’ç«‹æ‘†é—®é¢˜


ğŸ˜‹ğŸ˜‹å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®è¯ï¼š**torchkeras**ï¼Œè·å–æœ¬æ–‡notebookæºä»£ç å’Œæ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚


è¡¨æ ¼å‹æ–¹æ³•å­˜å‚¨çš„çŠ¶æ€æ•°é‡æœ‰é™ï¼Œå½“é¢å¯¹å›´æ£‹æˆ–æœºå™¨äººæ§åˆ¶è¿™ç±»æœ‰æ•°ä¸æ¸…çš„çŠ¶æ€çš„ç¯å¢ƒæ—¶ï¼Œè¡¨æ ¼å‹æ–¹æ³•åœ¨å­˜å‚¨å’ŒæŸ¥æ‰¾æ•ˆç‡ä¸Šéƒ½å—å±€é™ï¼ŒDQNçš„æå‡ºè§£å†³äº†è¿™ä¸€å±€é™ï¼Œ**ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼æ›¿ä»£Qè¡¨æ ¼**ã€‚

æœ¬è´¨ä¸ŠDQNè¿˜æ˜¯ä¸€ä¸ªQ-learningç®—æ³•ï¼Œæ›´æ–°æ–¹å¼ä¸€è‡´ã€‚ä¸ºäº†æ›´å¥½çš„æ¢ç´¢ç¯å¢ƒï¼ŒåŒæ ·çš„ä¹Ÿé‡‡ç”¨epsilon-greedyæ–¹æ³•è®­ç»ƒã€‚

åœ¨Q-learningçš„åŸºç¡€ä¸Šï¼ŒDQNæå‡ºäº†ä¸¤ä¸ªæŠ€å·§ä½¿å¾—Qç½‘ç»œçš„æ›´æ–°è¿­ä»£æ›´ç¨³å®šã€‚

* ç»éªŒå›æ”¾(Experience Replay): ä½¿ç”¨ä¸€ä¸ªç»éªŒæ± å­˜å‚¨å¤šæ¡ç»éªŒs,a,r,s'ï¼Œå†ä»ä¸­éšæœºæŠ½å–ä¸€æ‰¹æ•°æ®é€å»è®­ç»ƒã€‚

* å›ºå®šç›®æ ‡(Fixed Q-Target): å¤åˆ¶ä¸€ä¸ªå’ŒåŸæ¥Qç½‘ç»œç»“æ„ä¸€æ ·çš„Target-Qç½‘ç»œï¼Œç”¨äºè®¡ç®—Qç›®æ ‡å€¼ã€‚



### ä¸€ï¼Œå‡†å¤‡ç¯å¢ƒ

<!-- #region -->
gymæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ æµ‹è¯•ç¯å¢ƒï¼Œå¯ä»¥ç”¨makeåˆ›å»ºç¯å¢ƒã€‚

envå…·æœ‰reset,step,renderå‡ ä¸ªæ–¹æ³•ã€‚


**å€’ç«‹æ‘†é—®é¢˜** 

ç¯å¢ƒè®¾è®¡å¦‚ä¸‹ï¼š

å€’ç«‹æ‘†é—®é¢˜ç¯å¢ƒçš„çŠ¶æ€æ˜¯æ— é™çš„ï¼Œç”¨ä¸€ä¸ª4ç»´çš„å‘é‡è¡¨ç¤ºstate.

4ä¸ªç»´åº¦åˆ†åˆ«ä»£è¡¨å¦‚ä¸‹å«ä¹‰

* cartä½ç½®ï¼š-2.4 ~ 2.4
* carté€Ÿåº¦ï¼š-inf ~ inf
* poleè§’åº¦ï¼š-0.5 ï½ 0.5 ï¼ˆradianï¼‰
* poleè§’é€Ÿåº¦ï¼š-inf ~ inf

æ™ºèƒ½ä½“è®¾è®¡å¦‚ä¸‹ï¼š

æ™ºèƒ½ä½“çš„actionæœ‰ä¸¤ç§ï¼Œå¯èƒ½çš„å–å€¼2ç§ï¼š

* 0ï¼Œå‘å·¦
* 1ï¼Œå‘å³

å¥–åŠ±è®¾è®¡å¦‚ä¸‹ï¼š

æ¯ç»´æŒä¸€ä¸ªæ­¥éª¤ï¼Œå¥–åŠ±+1ï¼Œåˆ°è¾¾200ä¸ªæ­¥éª¤ï¼Œæ¸¸æˆç»“æŸã€‚

æ‰€ä»¥æœ€é«˜å¾—åˆ†ä¸º200åˆ†ã€‚

å€’ç«‹æ‘†é—®é¢˜å¸Œæœ›è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿå°½å¯èƒ½åœ°ç»´æŒå€’ç«‹æ‘†çš„å¹³è¡¡ã€‚

<!-- #endregion -->

```python
import gym 
import numpy as np 
import pandas as pd 
import time
import matplotlib
import matplotlib.pyplot as plt
from IPython import display

print("gym.__version__=",gym.__version__)


%matplotlib inline

#å¯è§†åŒ–å‡½æ•°ï¼š
def show_state(env, step, info=''):
    plt.figure(num=10086,dpi=100)
    plt.clf()
    plt.imshow(env.render())
    plt.title("step: %d %s" % (step, info))
    plt.axis('off')
    display.clear_output(wait=True)
    display.display(plt.gcf())
    plt.close()
    

env = gym.make('CartPole-v1',render_mode="rgb_array") # CartPole-v0: é¢„æœŸæœ€åä¸€æ¬¡è¯„ä¼°æ€»åˆ† > 180ï¼ˆæœ€å¤§å€¼æ˜¯200ï¼‰
env.reset()
action_dim = env.action_space.n   # CartPole-v0: 2
obs_shape = env.observation_space.shape   # CartPole-v0: (4,)


```

```python
env.reset()
done = False
step = 0
while not done:
    
    action = np.random.randint(0, 1)
    state,reward,done,truncated,info = env.step(action)
    step+=1
    print(state,reward)
    time.sleep(1.0)
    show_state(env,step=step)
    #print('step {}: action {}, state {}, reward {}, done {}, truncated {}, info {}'.format(\
    #        step, action, state, reward, done, truncated,info))
    
display.clear_output(wait=True)
```

```python

```

### äºŒï¼Œå®šä¹‰Agent 


DQNçš„æ ¸å¿ƒæ€æƒ³ä¸ºä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼æ›¿ä»£Qè¡¨æ ¼ã€‚

Model: æ¨¡å‹ç»“æ„, è´Ÿè´£æ‹Ÿåˆå‡½æ•° Q(s,a)ã€‚ä¸»è¦å®ç°forwardæ–¹æ³•ã€‚

Agent:æ™ºèƒ½ä½“ï¼Œè´Ÿè´£å­¦ä¹ å¹¶å’Œç¯å¢ƒäº¤äº’, è¾“å…¥è¾“å‡ºæ˜¯numpy.arrayå½¢å¼ã€‚æœ‰sample(å•æ­¥é‡‡æ ·), predict(å•æ­¥é¢„æµ‹), æœ‰predict_batch(æ‰¹é‡é¢„æµ‹), compute_loss(è®¡ç®—æŸå¤±), sync_target(å‚æ•°åŒæ­¥)ç­‰æ–¹æ³•ã€‚



```python
import torch 
from torch import nn
import torch.nn.functional as F
import copy 

class Model(nn.Module):
    def __init__(self, obs_dim, action_dim):
        
        # 3å±‚å…¨è¿æ¥ç½‘ç»œ
        super(Model, self).__init__()
        self.obs_dim = obs_dim
        self.action_dim = action_dim 
        self.fc1 = nn.Linear(obs_dim,32)
        self.fc2 = nn.Linear(32,16)
        self.fc3 = nn.Linear(16,action_dim)

    def forward(self, obs):
        # è¾“å…¥stateï¼Œè¾“å‡ºæ‰€æœ‰actionå¯¹åº”çš„Qï¼Œ[Q(s,a1), Q(s,a2), Q(s,a3)...]
        x = self.fc1(obs)
        x = torch.tanh(x)
        x = self.fc2(x)
        x = torch.tanh(x)
        Q = self.fc3(x)
        return Q
    
model = Model(4,2)
model_target = copy.deepcopy(model)

model.eval()
model.forward(torch.tensor([[0.2,0.1,0.2,0.0],[0.3,0.5,0.2,0.6]]))

model_target.eval() 
model_target.forward(torch.tensor([[0.2,0.1,0.2,0.0],[0.3,0.5,0.2,0.6]]))


```

```python
import torch 
from torch import nn 
import copy 

class DQNAgent(nn.Module):
    def __init__(self, model, 
        gamma=0.9,
        e_greed=0.1,
        e_greed_decrement=0.001
        ):
        super().__init__()
        
        self.model = model
        self.target_model = copy.deepcopy(model)
  
        self.gamma = gamma # reward çš„è¡°å‡å› å­ï¼Œä¸€èˆ¬å– 0.9 åˆ° 0.999 ä¸ç­‰
        
        self.e_greed = e_greed  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢
        self.e_greed_decrement = e_greed_decrement  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½
        
        self.global_step = 0
        self.update_target_steps = 200 # æ¯éš”200ä¸ªtraining stepså†æŠŠmodelçš„å‚æ•°å¤åˆ¶åˆ°target_modelä¸­
        
        
    def forward(self,obs):
        return self.model(obs)
    
    @torch.no_grad()
    def predict_batch(self, obs):
        """ ä½¿ç”¨self.modelç½‘ç»œæ¥è·å– [Q(s,a1),Q(s,a2),...]
        """
        self.model.eval()
        return self.forward(obs)
    
    
    #å•æ­¥éª¤é‡‡æ ·    
    def sample(self, obs):
        sample = np.random.rand()  # äº§ç”Ÿ0~1ä¹‹é—´çš„å°æ•°
        if sample < self.e_greed:
            action = np.random.randint(self.model.action_dim)  # æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«é€‰æ‹©
        else:
            action = self.predict(obs)  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        self.e_greed = max(
            0.01, self.e_greed - self.e_greed_decrement)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½
        return action
    
    #å•æ­¥éª¤é¢„æµ‹   
    def predict(self, obs):  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
        obs = np.expand_dims(obs, axis=0)
        tensor = torch.tensor(obs,dtype=torch.float32).to(self.model.fc1.weight.device)
        pred_Q = self.predict_batch(tensor)
        action = torch.argmax(pred_Q,1,keepdim=True).cpu().numpy()  
        action = np.squeeze(action)
        return action
    
    
    def sync_target(self):
        """ æŠŠ self.model çš„æ¨¡å‹å‚æ•°å€¼åŒæ­¥åˆ° self.target_model
        """
        self.target_model.load_state_dict(self.model.state_dict())
    

    def compute_loss(self, obs, action, reward, next_obs, done):
        
        # æ¯éš”200ä¸ªtraining stepsåŒæ­¥ä¸€æ¬¡modelå’Œtarget_modelçš„å‚æ•°
        if self.global_step % self.update_target_steps == 0:
            self.sync_target()
        self.global_step += 1
        
        
        # ä»target_modelä¸­è·å– max Q' çš„å€¼ï¼Œç”¨äºè®¡ç®—target_Q
        self.target_model.eval()
        next_pred_value = self.target_model(next_obs)
        best_value = torch.max(next_pred_value, dim = 1,keepdim=True).values 
        target = reward.reshape((-1,1)) + (
            torch.tensor(1.0) - done.reshape(-1,1)) * self.gamma * best_value
        
        #print("best_value",best_value.shape)
        #print("target",target.shape)

        # è·å–Qé¢„æµ‹å€¼
        self.model.train()
        pred_value = self.model(obs)  
        action_onehot = F.one_hot(action.reshape(-1),
                num_classes = self.model.action_dim).float()
        prediction = torch.sum(pred_value*action_onehot,dim= 1,keepdim=True)
        
        #print("pred_value",pred_value.shape)
        #print("action_onehot",action_onehot.shape)
        #print("prediction",prediction.shape)
        
        # è®¡ç®— Q(s,a) ä¸ target_Qçš„å‡æ–¹å·®ï¼Œå¾—åˆ°loss
        loss = F.smooth_l1_loss(target,prediction)
        return loss 

```

```python
agent = DQNAgent(model,gamma=0.9,e_greed=0.1,
                 e_greed_decrement=0.001) 

```

```python
agent.predict_batch(torch.tensor([[2.0,3.0,4.0,2.0],[1.0,2.0,3.0,4.0]]))

```

```python
loss = agent.compute_loss(torch.tensor([[2.0,3.0,4.0,2.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]),
          torch.tensor([[1],[0],[0]]),
          torch.tensor([[1.0],[1.0],[1.0]]),
         torch.tensor([[2.0,3.0,0.4,2.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]),
         torch.tensor(0.9))
print(loss)
```

```python

```

```python

```

### ä¸‰ï¼Œè®­ç»ƒAgent 

```python
import random
import collections
import numpy as np

LEARN_FREQ = 5 # è®­ç»ƒé¢‘ç‡ï¼Œä¸éœ€è¦æ¯ä¸€ä¸ªstepéƒ½learnï¼Œæ”’ä¸€äº›æ–°å¢ç»éªŒåå†learnï¼Œæé«˜æ•ˆç‡
MEMORY_SIZE = 2048    # replay memoryçš„å¤§å°ï¼Œè¶Šå¤§è¶Šå ç”¨å†…å­˜
MEMORY_WARMUP_SIZE = 512  # replay_memory é‡Œéœ€è¦é¢„å­˜ä¸€äº›ç»éªŒæ•°æ®ï¼Œå†å¼€å¯è®­ç»ƒ
BATCH_SIZE = 128   # æ¯æ¬¡ç»™agent learnçš„æ•°æ®æ•°é‡ï¼Œä»replay memoryéšæœºé‡Œsampleä¸€æ‰¹æ•°æ®å‡ºæ¥

```

```python
#ç»éªŒå›æ”¾
class ReplayMemory(object):
    def __init__(self, max_size):
        self.buffer = collections.deque(maxlen=max_size)

    # å¢åŠ ä¸€æ¡ç»éªŒåˆ°ç»éªŒæ± ä¸­
    def append(self, exp):
        self.buffer.append(exp)

    # ä»ç»éªŒæ± ä¸­é€‰å–Næ¡ç»éªŒå‡ºæ¥
    def sample(self, batch_size):
        mini_batch = random.sample(self.buffer, batch_size)
        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []

        for experience in mini_batch:
            s, a, r, s_p, done = experience
            obs_batch.append(s)
            action_batch.append(a)
            reward_batch.append(r)
            next_obs_batch.append(s_p)
            done_batch.append(done)

        return np.array(obs_batch).astype('float32'), \
            np.array(action_batch).astype('int64'), np.array(reward_batch).astype('float32'),\
            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')

    def __len__(self):
        return len(self.buffer)
    
```

```python

```

```python
from torch.utils.data import IterableDataset,DataLoader  
class MyDataset(IterableDataset):
    def __init__(self,env,agent,rpm,stage='train',size=200):
        self.env = env
        self.agent = agent 
        self.rpm = rpm if stage=='train' else None
        self.stage = stage
        self.size = size 
        
    def __iter__(self):
        obs,info = self.env.reset() # é‡ç½®ç¯å¢ƒ, é‡æ–°å¼€ä¸€å±€ï¼ˆå³å¼€å§‹æ–°çš„ä¸€ä¸ªepisodeï¼‰
        step = 0
        batch_reward_true = [] #è®°å½•çœŸå®çš„reward
        while True:
            step += 1
            action = self.agent.sample(obs) 
            next_obs, reward, done, _, _ = self.env.step(action) # ä¸ç¯å¢ƒè¿›è¡Œä¸€ä¸ªäº¤äº’
            batch_reward_true.append(reward)
            
            if self.stage=='train':
                self.rpm.append((obs, action, reward, next_obs, float(done)))
                if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):
                    #yield batch_obs, batch_action, batch_reward, batch_next_obs,batch_done
                    yield self.rpm.sample(BATCH_SIZE),sum(batch_reward_true)
                    batch_reward_true.clear()
            
            else:
                obs_batch = np.array([obs]).astype('float32')
                action_batch = np.array([action]).astype('int64')
                reward_batch = np.array([reward]).astype('float32')
                next_obs_batch = np.array([next_obs]).astype('float32')
                done_batch = np.array([float(done)]).astype('float32')
                batch_data = obs_batch,action_batch,reward_batch,next_obs_batch,done_batch
                yield batch_data,sum(batch_reward_true)
                batch_reward_true.clear()
            
    
            if self.stage =='train':
                next_action = self.agent.sample(next_obs) # è®­ç»ƒé˜¶æ®µä½¿ç”¨æ¢ç´¢ç­–ç•¥
            else:
                next_action = self.agent.predict(next_obs) # éªŒè¯é˜¶æ®µä½¿ç”¨æ¨¡å‹é¢„æµ‹ç»“æœ
 
            action = next_action
            obs = next_obs   

            if done:
                if self.stage=='train' and len(self.rpm)<MEMORY_WARMUP_SIZE: #ç¡®ä¿è®­ç»ƒä¸€æ¬¡
                    yield self.rpm.sample(len(self.rpm)),sum(batch_reward_true)
                    batch_reward_true.clear()
                    break
                else:
                    break
    def __len__(self):
        return self.size 
    

env = gym.make('CartPole-v1') 
rpm = ReplayMemory(MEMORY_SIZE)

ds_train = MyDataset(env,agent,rpm,stage='train',size=1000)
ds_val = MyDataset(env,agent,rpm,stage='val',size=200)

```

```python
#ReplayMemoryé¢„å­˜æ•°æ®
while len(ds_train.rpm)<MEMORY_WARMUP_SIZE:
    for data in ds_train:
        print(len(ds_train.rpm))
        
```

```python
def collate_fn(batch):
    samples,rewards = [x[0] for x in batch],[x[-1] for x in batch] 
    samples = [torch.from_numpy(np.concatenate([x[j] for x in samples])) for j in range(5)] 
    rewards = torch.from_numpy(np.array([sum(rewards)]).astype('float32'))
    return samples,rewards 

dl_train = DataLoader(ds_train,batch_size=1,collate_fn=collate_fn)
dl_val = DataLoader(ds_val,batch_size=1,collate_fn=collate_fn)

```

```python
for batch in dl_train:
    break
```

```python
import sys,datetime
from tqdm import tqdm
import numpy as np
from accelerate import Accelerator
from torchkeras import KerasModel
import pandas as pd 

from copy import deepcopy

class StepRunner:
    def __init__(self, net, loss_fn, accelerator=None, stage = "train", metrics_dict = None, 
                 optimizer = None, lr_scheduler = None
                 ):
        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage
        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler
        self.accelerator = accelerator if accelerator is not None else Accelerator()
    
    def __call__(self, batch):
        
        samples,reward = batch
        #torch_data = ([torch.from_numpy(x) for x in batch_data])
        loss = self.net.compute_loss(*samples)
        
        #backward()
        if self.optimizer is not None and self.stage=="train":
            self.accelerator.backward(loss)
            if self.accelerator.sync_gradients:
                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)
            self.optimizer.step()
            if self.lr_scheduler is not None:
                self.lr_scheduler.step()
            self.optimizer.zero_grad()
                
            
        #losses ï¼ˆor plain metricï¼‰
        step_losses = {self.stage+'_reward':reward.item(), 
                       self.stage+'_loss':loss.item()}
        
        #metrics (stateful metric)
        step_metrics = {}
        if self.stage=="train":
            if self.optimizer is not None:
                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']
            else:
                step_metrics['lr'] = 0.0
        return step_losses,step_metrics
    

class EpochRunner:
    def __init__(self,steprunner,quiet=False):
        self.steprunner = steprunner
        self.stage = steprunner.stage
        self.accelerator = steprunner.accelerator
        self.net = steprunner.net
        self.quiet = quiet
        
    def __call__(self,dataloader):
        dataloader.agent = self.net 
        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)
        loop = tqdm(enumerate(dataloader,start=1), 
                    total=n,
                    file=sys.stdout,
                    disable=not self.accelerator.is_local_main_process or self.quiet,
                    ncols=100
                   )
        epoch_losses = {}
        for step, batch in loop: 
            if step<n:
                step_losses,step_metrics = self.steprunner(batch)   
                step_log = dict(step_losses,**step_metrics)
                for k,v in step_losses.items():
                    epoch_losses[k] = epoch_losses.get(k,0.0)+v
                loop.set_postfix(**step_log) 
            else:
                break
            
        epoch_metrics = step_metrics
        epoch_metrics.update({self.stage+"_"+name:metric_fn.compute().item() 
                         for name,metric_fn in self.steprunner.metrics_dict.items()})
        epoch_losses = {k:v for k,v in epoch_losses.items()}
        epoch_log = dict(epoch_losses,**epoch_metrics)
        loop.set_postfix(**epoch_log)

        for name,metric_fn in self.steprunner.metrics_dict.items():
            metric_fn.reset()
            
        return epoch_log
    
KerasModel.StepRunner = StepRunner
KerasModel.EpochRunner = EpochRunner 

```

```python
keras_model = KerasModel(net= agent,loss_fn=None,
        optimizer=torch.optim.Adam(agent.model.parameters(),lr=1e-2))

dfhistory = keras_model.fit(train_data = dl_train,
    val_data=dl_val,
    epochs=600,
    ckpt_path='checkpoint.pt',
    patience=100,
    monitor='val_reward',
    mode='max',
    callbacks=None,
    plot= True,
    cpu=True)

```

```python

```

### å››ï¼Œè¯„ä¼°Agent 

```python
# è¯„ä¼° agent, è·‘ 3 æ¬¡ï¼Œæ€»rewardæ±‚å¹³å‡
def evaluate(env, agent, render=False):
    eval_reward = []
    for i in range(2):
        obs,info = env.reset()
        episode_reward = 0
        step=0
        while step<300:
            action = agent.predict(obs)  # é¢„æµ‹åŠ¨ä½œï¼Œåªé€‰æœ€ä¼˜åŠ¨ä½œ
            obs, reward, done, _, _ = env.step(action)
            episode_reward += reward
            if render:
                show_state(env,step,info='reward='+str(episode_reward))
            if done:
                break
            step+=1
        eval_reward.append(episode_reward)
    return np.mean(eval_reward)

```

```python
#ç›´è§‚æ˜¾ç¤ºåŠ¨ç”»
env = gym.make('CartPole-v1',render_mode="rgb_array") 

evaluate(env, agent, render=True)

```

```python

```

### äº”ï¼Œä¿å­˜Agent 

```python
torch.save(agent.state_dict(),'dqn_agent.pt')
```

```python

```

**å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœåœ¨torchkerasçš„ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åœ¨é¡¹ç›®ä¸­æäº¤issueã€‚

å¦‚æœæƒ³è¦è·å¾—æ›´å¿«çš„åé¦ˆæˆ–è€…ä¸å…¶ä»–torchkerasç”¨æˆ·å°ä¼™ä¼´è¿›è¡Œäº¤æµï¼Œ

å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ã€‚

![](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)

```python

```
