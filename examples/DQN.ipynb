{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946aa78d",
   "metadata": {},
   "source": [
    "### DQNè§£å†³å€’ç«‹æ‘†é—®é¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901585b-ae5e-45ac-8b38-fc996749807e",
   "metadata": {},
   "source": [
    "ğŸ˜‹ğŸ˜‹å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®è¯ï¼š**torchkeras**ï¼Œè·å–æœ¬æ–‡notebookæºä»£ç å’Œæ•°æ®é›†ä¸‹è½½é“¾æ¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6546c403",
   "metadata": {},
   "source": [
    "è¡¨æ ¼å‹æ–¹æ³•å­˜å‚¨çš„çŠ¶æ€æ•°é‡æœ‰é™ï¼Œå½“é¢å¯¹å›´æ£‹æˆ–æœºå™¨äººæ§åˆ¶è¿™ç±»æœ‰æ•°ä¸æ¸…çš„çŠ¶æ€çš„ç¯å¢ƒæ—¶ï¼Œè¡¨æ ¼å‹æ–¹æ³•åœ¨å­˜å‚¨å’ŒæŸ¥æ‰¾æ•ˆç‡ä¸Šéƒ½å—å±€é™ï¼ŒDQNçš„æå‡ºè§£å†³äº†è¿™ä¸€å±€é™ï¼Œ**ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼æ›¿ä»£Qè¡¨æ ¼**ã€‚\n",
    "\n",
    "æœ¬è´¨ä¸ŠDQNè¿˜æ˜¯ä¸€ä¸ªQ-learningç®—æ³•ï¼Œæ›´æ–°æ–¹å¼ä¸€è‡´ã€‚ä¸ºäº†æ›´å¥½çš„æ¢ç´¢ç¯å¢ƒï¼ŒåŒæ ·çš„ä¹Ÿé‡‡ç”¨epsilon-greedyæ–¹æ³•è®­ç»ƒã€‚\n",
    "\n",
    "åœ¨Q-learningçš„åŸºç¡€ä¸Šï¼ŒDQNæå‡ºäº†ä¸¤ä¸ªæŠ€å·§ä½¿å¾—Qç½‘ç»œçš„æ›´æ–°è¿­ä»£æ›´ç¨³å®šã€‚\n",
    "\n",
    "* ç»éªŒå›æ”¾(Experience Replay): ä½¿ç”¨ä¸€ä¸ªç»éªŒæ± å­˜å‚¨å¤šæ¡ç»éªŒs,a,r,s'ï¼Œå†ä»ä¸­éšæœºæŠ½å–ä¸€æ‰¹æ•°æ®é€å»è®­ç»ƒã€‚\n",
    "\n",
    "* å›ºå®šç›®æ ‡(Fixed Q-Target): å¤åˆ¶ä¸€ä¸ªå’ŒåŸæ¥Qç½‘ç»œç»“æ„ä¸€æ ·çš„Target-Qç½‘ç»œï¼Œç”¨äºè®¡ç®—Qç›®æ ‡å€¼ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0577e68",
   "metadata": {},
   "source": [
    "### ä¸€ï¼Œå‡†å¤‡ç¯å¢ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ab52c",
   "metadata": {},
   "source": [
    "gymæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„å¼ºåŒ–å­¦ä¹ æµ‹è¯•ç¯å¢ƒï¼Œå¯ä»¥ç”¨makeåˆ›å»ºç¯å¢ƒã€‚\n",
    "\n",
    "envå…·æœ‰reset,step,renderå‡ ä¸ªæ–¹æ³•ã€‚\n",
    "\n",
    "\n",
    "**å€’ç«‹æ‘†é—®é¢˜** \n",
    "\n",
    "ç¯å¢ƒè®¾è®¡å¦‚ä¸‹ï¼š\n",
    "\n",
    "å€’ç«‹æ‘†é—®é¢˜ç¯å¢ƒçš„çŠ¶æ€æ˜¯æ— é™çš„ï¼Œç”¨ä¸€ä¸ª4ç»´çš„å‘é‡è¡¨ç¤ºstate.\n",
    "\n",
    "4ä¸ªç»´åº¦åˆ†åˆ«ä»£è¡¨å¦‚ä¸‹å«ä¹‰\n",
    "\n",
    "* cartä½ç½®ï¼š-2.4 ~ 2.4\n",
    "* carté€Ÿåº¦ï¼š-inf ~ inf\n",
    "* poleè§’åº¦ï¼š-0.5 ï½ 0.5 ï¼ˆradianï¼‰\n",
    "* poleè§’é€Ÿåº¦ï¼š-inf ~ inf\n",
    "\n",
    "æ™ºèƒ½ä½“è®¾è®¡å¦‚ä¸‹ï¼š\n",
    "\n",
    "æ™ºèƒ½ä½“çš„actionæœ‰ä¸¤ç§ï¼Œå¯èƒ½çš„å–å€¼2ç§ï¼š\n",
    "\n",
    "* 0ï¼Œå‘å·¦\n",
    "* 1ï¼Œå‘å³\n",
    "\n",
    "å¥–åŠ±è®¾è®¡å¦‚ä¸‹ï¼š\n",
    "\n",
    "æ¯ç»´æŒä¸€ä¸ªæ­¥éª¤ï¼Œå¥–åŠ±+1ï¼Œåˆ°è¾¾200ä¸ªæ­¥éª¤ï¼Œæ¸¸æˆç»“æŸã€‚\n",
    "\n",
    "æ‰€ä»¥æœ€é«˜å¾—åˆ†ä¸º200åˆ†ã€‚\n",
    "\n",
    "å€’ç«‹æ‘†é—®é¢˜å¸Œæœ›è®­ç»ƒä¸€ä¸ªæ™ºèƒ½ä½“èƒ½å¤Ÿå°½å¯èƒ½åœ°ç»´æŒå€’ç«‹æ‘†çš„å¹³è¡¡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb8a6b3c-9070-4e12-bb7b-b488c3b46577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.insert(0,\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a129271d-c504-4a9e-b200-da1c80268d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchkeras import KerasModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283986f4-3585-4e22-9ed7-f20a29916424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "print(\"gym.__version__=\",gym.__version__)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#å¯è§†åŒ–å‡½æ•°ï¼š\n",
    "def show_state(env, step, info=''):\n",
    "    plt.figure(num=10086,dpi=100)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render())\n",
    "    plt.title(\"step: %d %s\" % (step, info))\n",
    "    plt.axis('off')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\") # CartPole-v0: é¢„æœŸæœ€åä¸€æ¬¡è¯„ä¼°æ€»åˆ† > 180ï¼ˆæœ€å¤§å€¼æ˜¯200ï¼‰\n",
    "env.reset()\n",
    "action_dim = env.action_space.n   # CartPole-v0: 2\n",
    "obs_shape = env.observation_space.shape   # CartPole-v0: (4,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bc64e-66a9-40a4-81bb-1d85e2cb8bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while not done:\n",
    "    \n",
    "    action = np.random.randint(0, 1)\n",
    "    state,reward,done,truncated,info = env.step(action)\n",
    "    step+=1\n",
    "    print(state,reward)\n",
    "    time.sleep(1.0)\n",
    "    show_state(env,step=step)\n",
    "    #print('step {}: action {}, state {}, reward {}, done {}, truncated {}, info {}'.format(\\\n",
    "    #        step, action, state, reward, done, truncated,info))\n",
    "    \n",
    "display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6b673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86a5c44a",
   "metadata": {},
   "source": [
    "### äºŒï¼Œå®šä¹‰Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff500b",
   "metadata": {},
   "source": [
    "DQNçš„æ ¸å¿ƒæ€æƒ³ä¸ºä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¿‘ä¼¼æ›¿ä»£Qè¡¨æ ¼ã€‚\n",
    "\n",
    "Model: æ¨¡å‹ç»“æ„, è´Ÿè´£æ‹Ÿåˆå‡½æ•° Q(s,a)ã€‚ä¸»è¦å®ç°forwardæ–¹æ³•ã€‚\n",
    "\n",
    "Agent:æ™ºèƒ½ä½“ï¼Œè´Ÿè´£å­¦ä¹ å¹¶å’Œç¯å¢ƒäº¤äº’, è¾“å…¥è¾“å‡ºæ˜¯numpy.arrayå½¢å¼ã€‚æœ‰sample(å•æ­¥é‡‡æ ·), predict(å•æ­¥é¢„æµ‹), æœ‰predict_batch(æ‰¹é‡é¢„æµ‹), compute_loss(è®¡ç®—æŸå¤±), sync_target(å‚æ•°åŒæ­¥)ç­‰æ–¹æ³•ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim):\n",
    "        \n",
    "        # 3å±‚å…¨è¿æ¥ç½‘ç»œ\n",
    "        super(Model, self).__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim \n",
    "        self.fc1 = nn.Linear(obs_dim,32)\n",
    "        self.fc2 = nn.Linear(32,16)\n",
    "        self.fc3 = nn.Linear(16,action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # è¾“å…¥stateï¼Œè¾“å‡ºæ‰€æœ‰actionå¯¹åº”çš„Qï¼Œ[Q(s,a1), Q(s,a2), Q(s,a3)...]\n",
    "        x = self.fc1(obs)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.fc2(x)\n",
    "        x = torch.tanh(x)\n",
    "        Q = self.fc3(x)\n",
    "        return Q\n",
    "    \n",
    "model = Model(4,2)\n",
    "model_target = copy.deepcopy(model)\n",
    "\n",
    "model.eval()\n",
    "model.forward(torch.tensor([[0.2,0.1,0.2,0.0],[0.3,0.5,0.2,0.6]]))\n",
    "\n",
    "model_target.eval() \n",
    "model_target.forward(torch.tensor([[0.2,0.1,0.2,0.0],[0.3,0.5,0.2,0.6]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a702800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "import copy \n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, model, \n",
    "        gamma=0.9,\n",
    "        e_greed=0.1,\n",
    "        e_greed_decrement=0.001\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = model\n",
    "        self.target_model = copy.deepcopy(model)\n",
    "  \n",
    "        self.gamma = gamma # reward çš„è¡°å‡å› å­ï¼Œä¸€èˆ¬å– 0.9 åˆ° 0.999 ä¸ç­‰\n",
    "        \n",
    "        self.e_greed = e_greed  # æœ‰ä¸€å®šæ¦‚ç‡éšæœºé€‰å–åŠ¨ä½œï¼Œæ¢ç´¢\n",
    "        self.e_greed_decrement = e_greed_decrement  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½\n",
    "        \n",
    "        self.global_step = 0\n",
    "        self.update_target_steps = 200 # æ¯éš”200ä¸ªtraining stepså†æŠŠmodelçš„å‚æ•°å¤åˆ¶åˆ°target_modelä¸­\n",
    "        \n",
    "        \n",
    "    def forward(self,obs):\n",
    "        return self.model(obs)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def predict_batch(self, obs):\n",
    "        \"\"\" ä½¿ç”¨self.modelç½‘ç»œæ¥è·å– [Q(s,a1),Q(s,a2),...]\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        return self.forward(obs)\n",
    "    \n",
    "    \n",
    "    #å•æ­¥éª¤é‡‡æ ·    \n",
    "    def sample(self, obs):\n",
    "        sample = np.random.rand()  # äº§ç”Ÿ0~1ä¹‹é—´çš„å°æ•°\n",
    "        if sample < self.e_greed:\n",
    "            action = np.random.randint(self.model.action_dim)  # æ¢ç´¢ï¼šæ¯ä¸ªåŠ¨ä½œéƒ½æœ‰æ¦‚ç‡è¢«é€‰æ‹©\n",
    "        else:\n",
    "            action = self.predict(obs)  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ\n",
    "        self.e_greed = max(\n",
    "            0.01, self.e_greed - self.e_greed_decrement)  # éšç€è®­ç»ƒé€æ­¥æ”¶æ•›ï¼Œæ¢ç´¢çš„ç¨‹åº¦æ…¢æ…¢é™ä½\n",
    "        return action\n",
    "    \n",
    "    #å•æ­¥éª¤é¢„æµ‹   \n",
    "    def predict(self, obs):  # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        tensor = torch.tensor(obs,dtype=torch.float32).to(self.model.fc1.weight.device)\n",
    "        pred_Q = self.predict_batch(tensor)\n",
    "        action = torch.argmax(pred_Q,1,keepdim=True).cpu().numpy()  \n",
    "        action = np.squeeze(action)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def sync_target(self):\n",
    "        \"\"\" æŠŠ self.model çš„æ¨¡å‹å‚æ•°å€¼åŒæ­¥åˆ° self.target_model\n",
    "        \"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "\n",
    "    def compute_loss(self, obs, action, reward, next_obs, done):\n",
    "        \n",
    "        # æ¯éš”200ä¸ªtraining stepsåŒæ­¥ä¸€æ¬¡modelå’Œtarget_modelçš„å‚æ•°\n",
    "        if self.global_step % self.update_target_steps == 0:\n",
    "            self.sync_target()\n",
    "        self.global_step += 1\n",
    "        \n",
    "        \n",
    "        # ä»target_modelä¸­è·å– max Q' çš„å€¼ï¼Œç”¨äºè®¡ç®—target_Q\n",
    "        self.target_model.eval()\n",
    "        next_pred_value = self.target_model(next_obs)\n",
    "        best_value = torch.max(next_pred_value, dim = 1,keepdim=True).values \n",
    "        target = reward.reshape((-1,1)) + (\n",
    "            torch.tensor(1.0) - done.reshape(-1,1)) * self.gamma * best_value\n",
    "        \n",
    "        #print(\"best_value\",best_value.shape)\n",
    "        #print(\"target\",target.shape)\n",
    "\n",
    "        # è·å–Qé¢„æµ‹å€¼\n",
    "        self.model.train()\n",
    "        pred_value = self.model(obs)  \n",
    "        action_onehot = F.one_hot(action.reshape(-1),\n",
    "                num_classes = self.model.action_dim).float()\n",
    "        prediction = torch.sum(pred_value*action_onehot,dim= 1,keepdim=True)\n",
    "        \n",
    "        #print(\"pred_value\",pred_value.shape)\n",
    "        #print(\"action_onehot\",action_onehot.shape)\n",
    "        #print(\"prediction\",prediction.shape)\n",
    "        \n",
    "        # è®¡ç®— Q(s,a) ä¸ target_Qçš„å‡æ–¹å·®ï¼Œå¾—åˆ°loss\n",
    "        loss = F.smooth_l1_loss(target,prediction)\n",
    "        return loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cded035-1714-48bf-a772-ab94d1f9e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(model,gamma=0.9,e_greed=0.1,\n",
    "                 e_greed_decrement=0.001) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf29876-7e8e-4f0a-b6f0-6bcb2b3fea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.predict_batch(torch.tensor([[2.0,3.0,4.0,2.0],[1.0,2.0,3.0,4.0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa50e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = agent.compute_loss(torch.tensor([[2.0,3.0,4.0,2.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]),\n",
    "          torch.tensor([[1],[0],[0]]),\n",
    "          torch.tensor([[1.0],[1.0],[1.0]]),\n",
    "         torch.tensor([[2.0,3.0,0.4,2.0],[1.0,2.0,3.0,4.0],[1.0,2.0,3.0,4.0]]),\n",
    "         torch.tensor(0.9))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109152db-ed87-4f35-80b1-61b99bd00c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93668346-2510-416b-b695-926b7a52074a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efcec74b",
   "metadata": {},
   "source": [
    "### ä¸‰ï¼Œè®­ç»ƒAgent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840ee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "LEARN_FREQ = 5 # è®­ç»ƒé¢‘ç‡ï¼Œä¸éœ€è¦æ¯ä¸€ä¸ªstepéƒ½learnï¼Œæ”’ä¸€äº›æ–°å¢ç»éªŒåå†learnï¼Œæé«˜æ•ˆç‡\n",
    "MEMORY_SIZE = 2048    # replay memoryçš„å¤§å°ï¼Œè¶Šå¤§è¶Šå ç”¨å†…å­˜\n",
    "MEMORY_WARMUP_SIZE = 512  # replay_memory é‡Œéœ€è¦é¢„å­˜ä¸€äº›ç»éªŒæ•°æ®ï¼Œå†å¼€å¯è®­ç»ƒ\n",
    "BATCH_SIZE = 128   # æ¯æ¬¡ç»™agent learnçš„æ•°æ®æ•°é‡ï¼Œä»replay memoryéšæœºé‡Œsampleä¸€æ‰¹æ•°æ®å‡ºæ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e7709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç»éªŒå›æ”¾\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = collections.deque(maxlen=max_size)\n",
    "\n",
    "    # å¢åŠ ä¸€æ¡ç»éªŒåˆ°ç»éªŒæ± ä¸­\n",
    "    def append(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "\n",
    "    # ä»ç»éªŒæ± ä¸­é€‰å–Næ¡ç»éªŒå‡ºæ¥\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []\n",
    "\n",
    "        for experience in mini_batch:\n",
    "            s, a, r, s_p, done = experience\n",
    "            obs_batch.append(s)\n",
    "            action_batch.append(a)\n",
    "            reward_batch.append(r)\n",
    "            next_obs_batch.append(s_p)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return np.array(obs_batch).astype('float32'), \\\n",
    "            np.array(action_batch).astype('int64'), np.array(reward_batch).astype('float32'),\\\n",
    "            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d2a816-2ae5-4395-bb48-c95460338f1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef47dae-5e61-4e69-9834-133e7fdf8c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset,DataLoader  \n",
    "class MyDataset(IterableDataset):\n",
    "    def __init__(self,env,agent,rpm,stage='train',size=200):\n",
    "        self.env = env\n",
    "        self.agent = agent \n",
    "        self.rpm = rpm if stage=='train' else None\n",
    "        self.stage = stage\n",
    "        self.size = size \n",
    "        \n",
    "    def __iter__(self):\n",
    "        obs,info = self.env.reset() # é‡ç½®ç¯å¢ƒ, é‡æ–°å¼€ä¸€å±€ï¼ˆå³å¼€å§‹æ–°çš„ä¸€ä¸ªepisodeï¼‰\n",
    "        step = 0\n",
    "        batch_reward_true = [] #è®°å½•çœŸå®çš„reward\n",
    "        while True:\n",
    "            step += 1\n",
    "            action = self.agent.sample(obs) \n",
    "            next_obs, reward, done, _, _ = self.env.step(action) # ä¸ç¯å¢ƒè¿›è¡Œä¸€ä¸ªäº¤äº’\n",
    "            batch_reward_true.append(reward)\n",
    "            \n",
    "            if self.stage=='train':\n",
    "                self.rpm.append((obs, action, reward, next_obs, float(done)))\n",
    "                if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):\n",
    "                    #yield batch_obs, batch_action, batch_reward, batch_next_obs,batch_done\n",
    "                    yield self.rpm.sample(BATCH_SIZE),sum(batch_reward_true)\n",
    "                    batch_reward_true.clear()\n",
    "            \n",
    "            else:\n",
    "                obs_batch = np.array([obs]).astype('float32')\n",
    "                action_batch = np.array([action]).astype('int64')\n",
    "                reward_batch = np.array([reward]).astype('float32')\n",
    "                next_obs_batch = np.array([next_obs]).astype('float32')\n",
    "                done_batch = np.array([float(done)]).astype('float32')\n",
    "                batch_data = obs_batch,action_batch,reward_batch,next_obs_batch,done_batch\n",
    "                yield batch_data,sum(batch_reward_true)\n",
    "                batch_reward_true.clear()\n",
    "            \n",
    "    \n",
    "            if self.stage =='train':\n",
    "                next_action = self.agent.sample(next_obs) # è®­ç»ƒé˜¶æ®µä½¿ç”¨æ¢ç´¢ç­–ç•¥\n",
    "            else:\n",
    "                next_action = self.agent.predict(next_obs) # éªŒè¯é˜¶æ®µä½¿ç”¨æ¨¡å‹é¢„æµ‹ç»“æœ\n",
    " \n",
    "            action = next_action\n",
    "            obs = next_obs   \n",
    "\n",
    "            if done:\n",
    "                if self.stage=='train' and len(self.rpm)<MEMORY_WARMUP_SIZE: #ç¡®ä¿è®­ç»ƒä¸€æ¬¡\n",
    "                    yield self.rpm.sample(len(self.rpm)),sum(batch_reward_true)\n",
    "                    batch_reward_true.clear()\n",
    "                    break\n",
    "                else:\n",
    "                    break\n",
    "    def __len__(self):\n",
    "        return self.size \n",
    "    \n",
    "\n",
    "env = gym.make('CartPole-v1') \n",
    "rpm = ReplayMemory(MEMORY_SIZE)\n",
    "\n",
    "ds_train = MyDataset(env,agent,rpm,stage='train',size=1000)\n",
    "ds_val = MyDataset(env,agent,rpm,stage='val',size=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5f16c-85b3-4e5d-9a2e-d5a3765a8b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ReplayMemoryé¢„å­˜æ•°æ®\n",
    "while len(ds_train.rpm)<MEMORY_WARMUP_SIZE:\n",
    "    for data in ds_train:\n",
    "        print(len(ds_train.rpm))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a73e72-ce44-4e47-8156-167a22e4c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    samples,rewards = [x[0] for x in batch],[x[-1] for x in batch] \n",
    "    samples = [torch.from_numpy(np.concatenate([x[j] for x in samples])) for j in range(5)] \n",
    "    rewards = torch.from_numpy(np.array([sum(rewards)]).astype('float32'))\n",
    "    return samples,rewards \n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size=1,collate_fn=collate_fn)\n",
    "dl_val = DataLoader(ds_val,batch_size=1,collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8980d2c-0ee9-404f-8d15-eb9ef2af6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl_train:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70436e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from torchkeras import KerasModel\n",
    "import pandas as pd \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        samples,reward = batch\n",
    "        #torch_data = ([torch.from_numpy(x) for x in batch_data])\n",
    "        loss = self.net.compute_loss(*samples)\n",
    "        \n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "                \n",
    "            \n",
    "        #losses ï¼ˆor plain metricï¼‰\n",
    "        step_losses = {self.stage+'_reward':reward.item(), \n",
    "                       self.stage+'_loss':loss.item()}\n",
    "        \n",
    "        #metrics (stateful metric)\n",
    "        step_metrics = {}\n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "\n",
    "class EpochRunner:\n",
    "    def __init__(self,steprunner,quiet=False):\n",
    "        self.steprunner = steprunner\n",
    "        self.stage = steprunner.stage\n",
    "        self.accelerator = steprunner.accelerator\n",
    "        self.net = steprunner.net\n",
    "        self.quiet = quiet\n",
    "        \n",
    "    def __call__(self,dataloader):\n",
    "        dataloader.agent = self.net \n",
    "        n = dataloader.size  if hasattr(dataloader,'size') else len(dataloader)\n",
    "        loop = tqdm(enumerate(dataloader,start=1), \n",
    "                    total=n,\n",
    "                    file=sys.stdout,\n",
    "                    disable=not self.accelerator.is_local_main_process or self.quiet,\n",
    "                    ncols=100\n",
    "                   )\n",
    "        epoch_losses = {}\n",
    "        for step, batch in loop: \n",
    "            if step<n:\n",
    "                step_losses,step_metrics = self.steprunner(batch)   \n",
    "                step_log = dict(step_losses,**step_metrics)\n",
    "                for k,v in step_losses.items():\n",
    "                    epoch_losses[k] = epoch_losses.get(k,0.0)+v\n",
    "                loop.set_postfix(**step_log) \n",
    "            else:\n",
    "                break\n",
    "            \n",
    "        epoch_metrics = step_metrics\n",
    "        epoch_metrics.update({self.stage+\"_\"+name:metric_fn.compute().item() \n",
    "                         for name,metric_fn in self.steprunner.metrics_dict.items()})\n",
    "        epoch_losses = {k:v for k,v in epoch_losses.items()}\n",
    "        epoch_log = dict(epoch_losses,**epoch_metrics)\n",
    "        loop.set_postfix(**epoch_log)\n",
    "\n",
    "        for name,metric_fn in self.steprunner.metrics_dict.items():\n",
    "            metric_fn.reset()\n",
    "            \n",
    "        return epoch_log\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner\n",
    "KerasModel.EpochRunner = EpochRunner \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49957914",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = KerasModel(net= agent,loss_fn=None,\n",
    "        optimizer=torch.optim.Adam(agent.model.parameters(),lr=1e-2))\n",
    "\n",
    "dfhistory = keras_model.fit(train_data = dl_train,\n",
    "    val_data=dl_val,\n",
    "    epochs=600,\n",
    "    ckpt_path='checkpoint.pt',\n",
    "    patience=100,\n",
    "    monitor='val_reward',\n",
    "    mode='max',\n",
    "    callbacks=None,\n",
    "    plot= True,\n",
    "    cpu=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b48153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "475e96d4",
   "metadata": {},
   "source": [
    "### å››ï¼Œè¯„ä¼°Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a68f2d-785b-432a-a93c-7af94a7c63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯„ä¼° agent, è·‘ 3 æ¬¡ï¼Œæ€»rewardæ±‚å¹³å‡\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(2):\n",
    "        obs,info = env.reset()\n",
    "        episode_reward = 0\n",
    "        step=0\n",
    "        while step<300:\n",
    "            action = agent.predict(obs)  # é¢„æµ‹åŠ¨ä½œï¼Œåªé€‰æœ€ä¼˜åŠ¨ä½œ\n",
    "            obs, reward, done, _, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                show_state(env,step,info='reward='+str(episode_reward))\n",
    "            if done:\n",
    "                break\n",
    "            step+=1\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2476b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç›´è§‚æ˜¾ç¤ºåŠ¨ç”»\n",
    "env = gym.make('CartPole-v1',render_mode=\"rgb_array\") \n",
    "\n",
    "evaluate(env, agent, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd6239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd03379b",
   "metadata": {},
   "source": [
    "### äº”ï¼Œä¿å­˜Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211be5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(),'dqn_agent.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fb3fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfa6498a-6a18-4084-9e9f-3ae566283a16",
   "metadata": {},
   "source": [
    "**å¦‚æœæœ¬é¡¹ç›®å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** \n",
    "\n",
    "å¦‚æœåœ¨torchkerasçš„ä½¿ç”¨ä¸­é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥åœ¨é¡¹ç›®ä¸­æäº¤issueã€‚\n",
    "\n",
    "å¦‚æœæƒ³è¦è·å¾—æ›´å¿«çš„åé¦ˆæˆ–è€…ä¸å…¶ä»–torchkerasç”¨æˆ·å°ä¼™ä¼´è¿›è¡Œäº¤æµï¼Œ\n",
    "\n",
    "å¯ä»¥åœ¨å…¬ä¼—å·ç®—æ³•ç¾é£Ÿå±‹åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ã€‚\n",
    "\n",
    "![](https://tva1.sinaimg.cn/large/e6c9d24egy1h41m2zugguj20k00b9q46.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7b8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md,ipynb",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
